{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by <module> at <ipython-input-1-1cb5f5e6034d>:13 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-c69a44d0c5b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark_home\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/python\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python/lib/py4j-0.8.2.1-src.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mexecfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python/pyspark/shell.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/rcoleman/spark/python/pyspark/shell.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.executor.uri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPARK_EXECUTOR_URI\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyFiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rcoleman/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/Users/rcoleman/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    259\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 261\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by <module> at <ipython-input-1-1cb5f5e6034d>:13 "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math, sys, os\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "# setup pyspark for IPython_notebooks\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Top-level RDD methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RDD = sc.parallelize(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect: \n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "\n",
      "take(15): \n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "\n",
      "takeSample(False,15)\n",
      "[651, 476, 772, 844, 743, 170, 576, 768, 187, 623, 841, 23, 145, 401, 128]\n",
      "\n",
      "isEmpty: \n",
      "False\n",
      "\n",
      "count: \n",
      "1000\n",
      "\n",
      "countApprox(timeout in seconds, confidence interval): \n",
      "1000\n",
      "\n",
      "min, max: \n",
      "0\n",
      "999\n",
      "\n",
      "sum\n",
      "499500\n",
      "\n",
      "mean: \n",
      "499.5\n",
      "\n",
      "meanApprox(timeout in seconds, confidence=0.95): \n",
      "499.5\n",
      "\n",
      "stdev\n",
      "288.674990257\n",
      "\n",
      "variance\n",
      "83333.25\n",
      "\n",
      "histogram(10)\n",
      "bins: \n",
      "[0.0, 99.9, 199.8, 299.70000000000005, 399.6, 499.5, 599.4000000000001, 699.3000000000001, 799.2, 899.1, 999]\n",
      "counts: \n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n"
     ]
    }
   ],
   "source": [
    "print \"collect: \"\n",
    "print RDD.collect()[:15]\n",
    "\n",
    "print \"\"\n",
    "print \"take(15): \"\n",
    "print RDD.take(15)\n",
    "\n",
    "print \"\"\n",
    "print \"takeSample(False,15)\"\n",
    "print RDD.takeSample(False,15)\n",
    "\n",
    "print \"\"\n",
    "print \"isEmpty: \"\n",
    "print RDD.isEmpty()\n",
    "\n",
    "print \"\"\n",
    "print \"count: \"\n",
    "print RDD.count()\n",
    "\n",
    "print \"\"\n",
    "print \"countApprox(timeout in seconds, confidence interval): \"\n",
    "print RDD.countApprox(1,0.95)\n",
    "\n",
    "print \"\"\n",
    "print \"min, max: \"\n",
    "print RDD.min()\n",
    "print RDD.max()\n",
    "\n",
    "print \"\"\n",
    "print \"sum\"\n",
    "print RDD.sum()\n",
    "\n",
    "print \"\"\n",
    "print \"mean: \"\n",
    "print RDD.mean()\n",
    "\n",
    "print \"\"\n",
    "print \"meanApprox(timeout in seconds, confidence=0.95): \"\n",
    "print RDD.meanApprox(1, confidence=0.95)\n",
    "\n",
    "print \"\"\n",
    "print \"stdev\"\n",
    "print RDD.stdev()\n",
    "\n",
    "print \"\"\n",
    "print \"variance\"\n",
    "print RDD.variance()\n",
    "\n",
    "\n",
    "print \"\"\n",
    "print \"histogram(10)\"\n",
    "bins, counts = RDD.histogram(10)\n",
    "print \"bins: \"\n",
    "print bins\n",
    "print \"counts: \"\n",
    "print counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher-Order Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD.filter( lambda x: x > 990).collect()\n",
      "\n",
      "RDD.map(lambda x: x % 4).take(10)\n",
      "[0, 1, 2, 3, 0, 1, 2, 3, 0, 1]\n",
      "\n",
      "RDD.reduce(lambda a, b: min(a, b))\n",
      "0\n",
      "RDD.reduce(lambda a, b: max(a, b))\n",
      "999\n",
      "\n",
      "RDD.filter(lambda x: x == 2).collect()\n",
      "[4]\n",
      "RDD.flatMap(lambda x: [x] * (x % 5)).filter(lambda x: x == 4).collect()\n",
      "[4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "# fitler\n",
    "print \"RDD.filter( lambda x: x > 990).collect()\"\n",
    "RDD.filter( lambda x: x > 990).collect()\n",
    "\n",
    "print \"\"\n",
    "print \"RDD.map(lambda x: x % 4).take(10)\"\n",
    "print RDD.map(lambda x: x % 4).take(10)\n",
    "\n",
    "print \"\"\n",
    "print \"RDD.reduce(lambda a, b: min(a, b))\"\n",
    "print RDD.reduce(lambda a, b: min(a, b))\n",
    "print \"RDD.reduce(lambda a, b: max(a, b))\"\n",
    "print RDD.reduce(lambda a, b: max(a, b))\n",
    "\n",
    "print \"\"\n",
    "print \"RDD.filter(lambda x: x == 2).collect()\"\n",
    "print RDD.filter(lambda x: x == 4).collect()\n",
    "print \"RDD.flatMap(lambda x: [x] * (x % 5)).filter(lambda x: x == 4).collect()\"\n",
    "print RDD.flatMap(lambda x: [x] * (x % 5)).filter(lambda x: x == 4).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\n",
      "[('a', 0), ('b', 1), ('c', 2), ('d', 3), ('e', 4), ('f', 5), ('g', 6), ('h', 7), ('i', 8), ('j', 9), ('k', 10), ('l', 11), ('m', 12), ('n', 13), ('o', 14), ('p', 15), ('q', 16), ('r', 17), ('s', 18), ('t', 19), ('u', 20), ('v', 21), ('w', 22), ('x', 23), ('y', 24), ('z', 25)]\n",
      "\n",
      "RDD.collect() == zip(alpha,range(26))\n",
      "True\n",
      "\n",
      "cartesian RDD size: \n",
      "676\n",
      "\n",
      "subtract (set difference)\n",
      "[('x', 1), ('y', 4), ('y', 5)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "alpha = string.ascii_lowercase\n",
    "num_RDD = sc.parallelize(range(26))\n",
    "alpha_RDD = sc.parallelize(alpha)\n",
    "\n",
    "# zipping two rdds\n",
    "alpha_num_RDD = alpha_RDD.zip(num_RDD)\n",
    "\n",
    "print alpha_RDD.collect()\n",
    "print \"\"\n",
    "print num_RDD.collect()\n",
    "print \"\"\n",
    "print alpha_num_RDD.collect()\n",
    "print \"\"\n",
    "print \"RDD.collect() == zip(alpha,range(26))\"\n",
    "print alpha_num_RDD.collect() == zip(alpha,range(26))\n",
    "\n",
    "# cartesian product RDD\n",
    "# for every item in A pair with every item in B.\n",
    "# For RDDs of size N and M produces a new RDD of size N*M\n",
    "print \"\"\n",
    "cart_RDD = alpha_RDD.cartesian(num_RDD)\n",
    "print \"cartesian RDD size: \"\n",
    "print cart_RDD.count()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = sc.parallelize([('x', 1), ('y', 4), ('y', 5), ('x', 2)])\n",
      "sc.parallelize([('x', 2), ('y', 5), ('z', 6)])\n",
      "\n",
      "A.union(B).collect()\n",
      "[('x', 1), ('x', 2), ('x', 2), ('y', 4), ('y', 5), ('y', 5), ('z', 6)]\n",
      "\n",
      "union (distict)\n",
      "A.union(B).distinct().collect()\n",
      "[('x', 1), ('x', 2), ('y', 4), ('y', 5), ('z', 6)]\n",
      "\n",
      "intersection\n",
      "A.intersection(B).collect()\n",
      "[('x', 2), ('y', 5)]\n",
      "\n",
      "subtract (set difference)\n",
      "A.subtract(B).collect()\n",
      "[('x', 1), ('y', 4)]\n"
     ]
    }
   ],
   "source": [
    "print \"A = sc.parallelize([('x', 1), ('y', 4), ('y', 5), ('x', 2)])\"\n",
    "print \"sc.parallelize([('x', 2), ('y', 5), ('z', 6)])\"\n",
    "A = sc.parallelize([(\"x\", 1), (\"y\", 4), (\"y\", 5), (\"x\", 2)])\n",
    "B = sc.parallelize([(\"x\", 2), (\"y\", 5), (\"z\", 6)])\n",
    "\n",
    "print \"\"\n",
    "print \"A.union(B).collect()\"\n",
    "print sorted(A.union(B).collect())\n",
    "\n",
    "print \"\"\n",
    "print \"union (distict)\"\n",
    "print \"A.union(B).distinct().collect()\"\n",
    "print sorted(A.union(B).distinct().collect())\n",
    "\n",
    "print \"\"\n",
    "print \"intersection\"\n",
    "print \"A.intersection(B).collect()\"\n",
    "print sorted(A.intersection(B).collect())\n",
    "\n",
    "print \"\"\n",
    "print \"subtract (set difference)\"\n",
    "print \"A.subtract(B).collect()\"\n",
    "print sorted(A.subtract(B).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key-Value basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyBy\n",
      "RDD.keyBy(lambda x: x % 5).take(10)\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (0, 5), (1, 6), (2, 7), (3, 8), (4, 9)]\n",
      "\n",
      "keys\n",
      "RDD.keyBy(lambda x: x % 5).keys().distinct().count()\n",
      "5\n",
      "\n",
      "values\n",
      "RDD.keyBy(lambda x: x % 5).values().distinct().count()\n",
      "1000\n",
      "\n",
      "collectAsMap: \n",
      "{'a': 0, 'c': 2, 'b': 1, 'e': 4, 'd': 3, 'g': 6, 'f': 5, 'i': 8, 'h': 7, 'k': 10, 'j': 9, 'm': 12, 'l': 11, 'o': 14, 'n': 13, 'q': 16, 'p': 15, 's': 18, 'r': 17, 'u': 20, 't': 19, 'w': 22, 'v': 21, 'y': 24, 'x': 23, 'z': 25}\n",
      "\n",
      "countByKey\n",
      "{'a': 26, 'c': 26, 'b': 26, 'e': 26, 'd': 26, 'g': 26, 'f': 26, 'i': 26, 'h': 26, 'k': 26, 'j': 26, 'm': 26, 'l': 26, 'o': 26, 'n': 26, 'q': 26, 'p': 26, 's': 26, 'r': 26, 'u': 26, 't': 26, 'w': 26, 'v': 26, 'y': 26, 'x': 26, 'z': 26}\n",
      "\n",
      "reduceByKey\n",
      "RDD.keyBy(lambda x: x % 5).reduceByKey(lambda a, b: max(a,b)).collectAsMap()\n",
      "{0: 995, 1: 996, 2: 997, 3: 998, 4: 999}\n"
     ]
    }
   ],
   "source": [
    "print \"keyBy\"\n",
    "print \"RDD.keyBy(lambda x: x % 5).take(10)\"\n",
    "print RDD.keyBy(lambda x: x % 5).take(10)\n",
    "\n",
    "print \"\"\n",
    "print \"keys\"\n",
    "print \"RDD.keyBy(lambda x: x % 5).keys().distinct().count()\"\n",
    "print RDD.keyBy(lambda x: x % 5).keys().distinct().count()\n",
    "\n",
    "print \"\"\n",
    "print \"values\"\n",
    "print \"RDD.keyBy(lambda x: x % 5).values().distinct().count()\"\n",
    "print RDD.keyBy(lambda x: x % 5).values().distinct().count()\n",
    "\n",
    "print \"\"\n",
    "print \"collectAsMap \"\n",
    "print alpha_num_RDD.collectAsMap()\n",
    "\n",
    "print \"\"\n",
    "print \"countByKey\"\n",
    "print dict(cart_RDD.countByKey())\n",
    "\n",
    "print \"\"\n",
    "print \"reduceByKey\"\n",
    "print \"RDD.keyBy(lambda x: x % 5).reduceByKey(lambda a, b: max(a,b)).collectAsMap()\"\n",
    "print RDD.keyBy(lambda x: x % 5).reduceByKey(lambda a, b: max(a,b)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
