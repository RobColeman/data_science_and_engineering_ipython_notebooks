{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Batch GD\n",
    "\n",
    "Vanilla batch GD computes the gradient of the cost function wrt the parameters for the enitre dataset\n",
    "\n",
    "theta = theta - alpha * grad(theta)\n",
    "\n",
    "### Stocastic GD\n",
    "\n",
    "Stochastic GD computes the gradient of the cost function wrt the parameters for each data example, and applies them as updates one at a time, step-by-step.\n",
    "\n",
    "theta = theta - alpha * grad(theta, x_i, y_i)\n",
    "\n",
    "### Batch GD\n",
    "\n",
    "Batch Stochastic GD computes the gradient \"\" for a batch of the data, iteratively over all batches.\n",
    "\n",
    "theta = theta - alpha * grad(theta, x_i:x_i+bn, y_i:y_i+bn), where bn is the batch size\n",
    "\n",
    "- This reduces the variance of the parameter updates, which can lead to more stable learning\n",
    "- can utilize highly optimized matrix operations for learning, speeding up the compute operations\n",
    "- This is the method of choice when one mentions SDG, \n",
    "\n",
    "### Momentum\n",
    "\n",
    "Combine the last gradient direction with the current gradient to further stablize the gradient direction.\n",
    "\n",
    "vt = y*vt-1 + alpha * grad(theta, x, y)\n",
    "theta = theta - vt\n",
    "\n",
    "\n",
    "### Nesterov Accelerated Gradients\n",
    "\n",
    "This method gives the model a sense if where it will be, wrt momentum, for computing the next gradients.  We move the paramaters by the amount we would apply the momentum when computing the gradient of the cost function wrt the params.\n",
    "\n",
    "vt = y*vt-1 + alpha * grad(theta - y*vt-1, x, y)\n",
    "theta = theta - vt\n",
    "\n",
    "notice we're computing the gradient at theta position \"theta - y*vt-1\", which includes the momentum at this step.\n",
    "\n",
    "### Adagrad\n",
    "\n",
    "Adagrad adapts updates wrt how often they are updated.  Dimensions which see infrequent updates take larger steps, while dimensions which see frequent updates take smaller steps.  \n",
    "\n",
    "- It is well suited to deal with sparse data\n",
    "\n",
    "\n",
    "g_t,i = grad(theta_i) at step t\n",
    "\n",
    "theta_t+1,i = theta_t,i - alpha * g_t,i\n",
    "\n",
    "We have to normalize alpha wrt each parameter i\n",
    "\n",
    "theta_t+1,i = theta_t,i - alpha / adagrad_norm(g_t,i,i, eps) * g_t,i\n",
    "\n",
    "### Adadelta\n",
    "\n",
    "An extension of adagrad which further normalizes, and stabalizes the adagrad norm, and correct for the aggressive, monotonically decreasing gradient size.  It does so by reducing the adagrad adjustment to a fixed length window, and an exponentially decaying function of the squared parameter updates.\n",
    "\n",
    "### RMSProp\n",
    "\n",
    "Developed independantly of adadelta, also corrects for the decaying gradient problem of adagrad.  Like adadelta it applied an exponentially decaying function to the historic parameter updates.\n",
    "\n",
    "### Adam\n",
    "\n",
    "Adam also computes adaptive learning rates for individual parameters.  It does so with an exponentially decaying average of both the past updates and past gradients.\n",
    "\n",
    "### Nadam\n",
    "\n",
    "Incorporates Nesterov's Accelerating Gradients into Adam.\n",
    "\n",
    "### Parallelism and Hogwild\n",
    "\n",
    "- Hogwild apples to parse data, and depends on low collision rate to run in parallel.\n",
    "- Downpour SDG runs copies of the model to perform updates in parallel over batches.  Updates are shared with a parameter server, from which the parallel models also pull updates.  Models are at risk of diverging.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
