{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'urllib2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-44ae86747051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msettings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhttplib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'urllib2'"
     ]
    }
   ],
   "source": [
    "## broken/WIP\n",
    "## stolen from https://bitbucket.org/deshan/simple-web-crawler\n",
    "## using as blueprint but will rob-ify\n",
    "\n",
    "import urllib2\n",
    "import re \n",
    "import settings \n",
    "import httplib\n",
    "import files\n",
    "import uuid\n",
    "from urlparse import urlparse\n",
    "from os.path import join\n",
    "\n",
    "VALID_MIME_TYPES={'text/html','text/plain','text/xml','text/csv'}\n",
    "STATUS_CODES={'200','302'}\n",
    "BASE_SAVE_PATH=\"./save\"\n",
    "DEFAULT_FILE_FOLDER=\"files\"\n",
    "DEFAULT_MAIL_FILE=\"mails.txt\"\n",
    "\n",
    "linkRgx =re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', re.IGNORECASE)\n",
    "mimeTypeRgx=re.compile(r'^[A-Z,a-z]+[A-Z,a-z,1-9,]*/[A-Z,a-z,1-9]+')\n",
    "\n",
    "class Crawler: \n",
    "    \n",
    "    fileSavePath=''\n",
    "    basePath=''\n",
    "    seed=''\n",
    "    depth=0\n",
    "\n",
    "    \n",
    "    def __init__(self,seed,depth):\n",
    "        print('############################################################################')\n",
    "        print('Crawling started.')\n",
    "        print('Files are saved at path :'+self.basePath)\n",
    "        parseResult=urlparse(seed)\n",
    "        self.basePath=join(settings.BASE_SAVE_PATH,parseResult.netloc)\n",
    "        self.fileSavePath=join(self.basePath,settings.DEFAULT_FILE_FOLDER)\n",
    "        print( self.fileSavePath )\n",
    "        self.seed=seed\n",
    "        self.depth=depth\n",
    "        \n",
    "    def getBasePath(self):\n",
    "        return self.basePath\n",
    "    \n",
    "    def getFileSaveth(self):\n",
    "        return self.fileSavePath\n",
    "         \n",
    "    def getResponseHeader(self,url):\n",
    "        urlParse=urlparse(url)\n",
    "        conn = httplib.HTTPConnection(urlParse.netloc)\n",
    "        conn.request(\"HEAD\",urlParse.path)\n",
    "        res = conn.getresponse()\n",
    "        return res\n",
    "    \n",
    "    def downloadFile(self,url):\n",
    "        res = self.getResponse(url)\n",
    "        data=res.read()\n",
    "        filename = uuid.uuid4()\n",
    "        files.saveFile(data, str(filename), self.fileSavePath)\n",
    "        return data\n",
    "       \n",
    "    def getResponse(self,url):\n",
    "        res = urllib2.urlopen(url)\n",
    "        return res\n",
    "    \n",
    "    def harvestLinks(self,text):\n",
    "        links = set()\n",
    "        if text is not  None:\n",
    "            found=linkRgx.findall(text)\n",
    "            links.update(found)      \n",
    "        return links\n",
    "    \n",
    "    def isValidLink(self,url):\n",
    "        res=self.getResponseHeader(url)\n",
    "        if res is not None and str(res.status) in settings.STATUS_CODES:\n",
    "            head=res.getheader(\"Content-Type\")\n",
    "            if head is not None:\n",
    "                mimeType=mimeTypeRgx.match(head).group(0)\n",
    "                print( '\\tContent type:'+mimeType )\n",
    "                if mimeType  in settings.VALID_MIME_TYPES:\n",
    "                    return True      \n",
    "        return False\n",
    "              \n",
    "    def start(self):\n",
    "        self.crawl([self.seed], self.depth)\n",
    "        \n",
    "    def crawl(self,url,depth):\n",
    "        tocrawl=set()\n",
    "        tocrawl.update(url)\n",
    "        newLinks=set()\n",
    "        count=0\n",
    "        \n",
    "        print( '==================================================================================')\n",
    "        print( 'Depth :'+str(depth))\n",
    "        print( '==================================================================================')\n",
    "        while tocrawl:\n",
    "            page=str(tocrawl.pop())\n",
    "            count=count+1\n",
    "            print ('['+str(count)+']'+'------------------------------------------')\n",
    "            try:       \n",
    "                print( 'Checking Link:'+page+'...' )\n",
    "                if self.isValidLink(page):\n",
    "                    print( '\\tCrawling...')\n",
    "                    print( '\\tDownloading..' )\n",
    "                    content=self.downloadFile(page)\n",
    "                    print( '\\tDownloading completed')\n",
    "                    print( '\\tHarvesting links...')\n",
    "                    foundLinks=self.harvestLinks(content);\n",
    "                    newLinks.update(foundLinks)\n",
    "                    print( '\\tHarvesting links completed')\n",
    "                    print(  '\\tCrawling completed-'+str(len(foundLinks))+' links found')\n",
    "                    \n",
    "                else:\n",
    "                    print( 'Skipped Error Url:'+page )\n",
    "                    continue\n",
    "            except :\n",
    "                print( 'Error Crawling link:'+page )\n",
    "            \n",
    "              \n",
    "        if(depth>0):\n",
    "            self.crawl(newLinks, depth=depth-1)  \n",
    "        else:\n",
    "            return     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  os.path import isdir,join,isfile,exists\n",
    "from os import listdir,makedirs\n",
    "import pickle\n",
    "\n",
    "def getFiles(path):\n",
    "    onlyFiles=''\n",
    "    if isdir(path):\n",
    "        onlyFiles=[]\n",
    "        onlyFiles = [ join(path,f) for f in listdir(path) if isfile(join(path,f)) ]        \n",
    "    return onlyFiles\n",
    "      \n",
    "def saveFile(data,filename,path):\n",
    "    if isdir(path):\n",
    "        savePath=join(path,filename)\n",
    "        newfile = open(savePath, 'w+')\n",
    "        newfile.write(data)\n",
    "        newfile.close()\n",
    "        \n",
    "    else:\n",
    "        createDirectory(path)   \n",
    "        saveFile(data,filename,path)\n",
    "        \n",
    "def createDirectory(directory):\n",
    "    if not exists(directory):\n",
    "        makedirs(directory)       \n",
    "        \n",
    "def saveListToFile(mailList,filename,path):\n",
    "    if isdir(path):\n",
    "        savePath=join(path,filename)\n",
    "        newfile = open(savePath, 'w+')\n",
    "        for line in mailList:\n",
    "            newfile.write(str(line)+'\\n')\n",
    "        newfile.close()\n",
    "    else:\n",
    "        createDirectory(path)   \n",
    "        saveListToFile(mailList,filename,path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-9-246ab4be1c05>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-246ab4be1c05>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    path=cr.getBasePath()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import crawler\n",
    "import MailExtracter\n",
    "\n",
    "# seedUrl = start point url\n",
    "seedUrl='http://www.google.lk/'\n",
    "\n",
    "#depth = how deep web page should be crawled\n",
    "depth=0;\n",
    "\n",
    "crawl=True\n",
    "exMails=True\n",
    "\n",
    "#crawl\n",
    "if crawl:\n",
    "    cr= crawler.Crawler(seedUrl,depth)\n",
    "    cr.start()\n",
    "     path=cr.getBasePath()\n",
    "    \n",
    "#extract mails\n",
    " if exMails:\n",
    "     me=MailExtracter.MailExtracter(path)\n",
    "     me.start()\n",
    "     me.printMailList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-1-57dac3e73c30>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-57dac3e73c30>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    print \"Unit tests have failed!\"\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "from distutils.core import setup\n",
    "import unittest\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"Unknown distribution option\")\n",
    "\n",
    "import sys\n",
    "# patch distutils if it can't cope with the \"classifiers\" keyword\n",
    "if sys.version < '2.2.3':\n",
    "    from distutils.dist import DistributionMetadata\n",
    "    DistributionMetadata.classifiers = None\n",
    "    DistributionMetadata.download_url = None\n",
    "\n",
    "from BeautifulSoup import __version__\n",
    "\n",
    "#Make sure all the tests complete.\n",
    "import BeautifulSoupTests\n",
    "loader = unittest.TestLoader()\n",
    "result = unittest.TestResult()\n",
    "suite = loader.loadTestsFromModule(BeautifulSoupTests)\n",
    "suite.run(result)\n",
    "if not result.wasSuccessful():\n",
    "    print \"Unit tests have failed!\"\n",
    "    for l in result.errors, result.failures:\n",
    "        for case, error in l:\n",
    "            print \"-\" * 80\n",
    "            desc = case.shortDescription()\n",
    "            if desc:\n",
    "                print desc\n",
    "            print error        \n",
    "    print '''If you see an error like: \"'ascii' codec can't encode character...\", see\\nthe Beautiful Soup documentation:\\n http://www.crummy.com/software/BeautifulSoup/documentation.html#Why%20can't%20Beautiful%20Soup%20print%20out%20the%20non-ASCII%20characters%20I%20gave%20it?'''\n",
    "    print \"This might or might not be a problem depending on what you plan to do with\\nBeautiful Soup.\"\n",
    "    if sys.argv[1] == 'sdist':\n",
    "        print\n",
    "        print \"I'm not going to make a source distribution since the tests don't pass.\"\n",
    "        sys.exit(1)\n",
    "\n",
    "setup(name=\"BeautifulSoup\",\n",
    "      version=__version__,\n",
    "      py_modules=['BeautifulSoup', 'BeautifulSoupTests'],\n",
    "      description=\"HTML/XML parser for quick-turnaround applications like screen-scraping.\",\n",
    "      author=\"Leonard Richardson\",\n",
    "      author_email = \"leonardr@segfault.org\",\n",
    "      long_description=\"\"\"Beautiful Soup parses arbitrarily invalid SGML and provides a variety of methods and Pythonic idioms for iterating and searching the parse tree.\"\"\",\n",
    "      classifiers=[\"Development Status :: 5 - Production/Stable\",\n",
    "                   \"Intended Audience :: Developers\",\n",
    "                   \"License :: OSI Approved :: Python Software Foundation License\",\n",
    "                   \"Programming Language :: Python\",\n",
    "                   \"Topic :: Text Processing :: Markup :: HTML\",\n",
    "                   \"Topic :: Text Processing :: Markup :: XML\",\n",
    "                   \"Topic :: Text Processing :: Markup :: SGML\",\n",
    "                   \"Topic :: Software Development :: Libraries :: Python Modules\",\n",
    "                   ],\n",
    "      url=\"http://www.crummy.com/software/BeautifulSoup/\",\n",
    "      license=\"BSD\",\n",
    "      download_url=\"http://www.crummy.com/software/BeautifulSoup/download/\"\n",
    "      )\n",
    "    \n",
    "    # Send announce to:\n",
    "    #   python-announce@python.org\n",
    "    #   python-list@python.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In pseudocode\n",
    "\n",
    "We do unordered BFS to crawl.\n",
    "\n",
    "- We initialize a hashtable of to_visit_table; our queue.\n",
    "- We initialize a hashtable of pages visited_table.\n",
    "- We insert a first page into the queue.\n",
    "- We select the next page in the queue, we first remove it from the hashtable, and insert it into the pages visited table.\n",
    "- We visit the page, and store the page however we want to.\n",
    "- Scan the page for links.\n",
    "- For all links in the page, we check the pages visited, if not in visited_table, we insert it into the to_visit_table.\n",
    "- Then we return to the queue for the next page to visit.\n",
    "\n",
    "We can parallelize the crawl by using agents to process the queue.\n",
    "\n",
    "The actors will have states.\n",
    "\n",
    "- visiting_page\n",
    "- waiting\n",
    "\n",
    "Our parent process will end when all of the child actors are in the waiting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
