{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Design the twitter app and search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outline use cases\n",
    "\n",
    "- user posts a tweet\n",
    "- user views their history of posts\n",
    "- user views another user's history of posts\n",
    "- user views their homepage, or mixed timeline of users they follow\n",
    "- user searches for another user\n",
    "- user follows another user\n",
    "- user searches for keywords\n",
    "\n",
    "extra functionality\n",
    "\n",
    "- user posts tweet referencing a tweet (re-tweet)\n",
    "- user posts a tweet referencing another user (@)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints, assumptions, volume\n",
    "\n",
    "### volume and constraings\n",
    "- Traffic is non-uniform, there are very popular and less popular people\n",
    "- Posting a tweet should be fast, arriving in followers feeds very quickly, unless you have millions of followers\n",
    "- 100M active users\n",
    "- 500M tweets a day (15B tweets per month)\n",
    "- each tweet averages 10 destination feeds (beware of this average, again non-uniform follower numbers).  5B tweets to arrive in feeds each day.\n",
    "- 250B feed read requests a month (~8.5B a day)\n",
    "- 10B searches a month\n",
    "- tweets can contain media (image, links, video links)\n",
    "\n",
    "We can see that this application is far more read-heavy than write heavy, so feed construction and update should be very fast, perhaps with real-time alerts for new arrivals.\n",
    "\n",
    "\n",
    "### Assumptions/ clairifying questions\n",
    "\n",
    "- how do we prioritize our feed.  The simplest form is strictly by time (maybe an option by user).\n",
    "\n",
    "We should consider placeholders for monitization here.  Sponsored tweets may have placeholders in the feed, to be filled dynamically at read time.  \n",
    "\n",
    "Even if we're easy on ourselves here, and pre-generate feeds for constant lookup, when updating the feed in the back-end we'll be dealing with asynchronous arrival of new tweets.  We will need to do a little sorting before update.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data size\n",
    "\n",
    "Per tweet\n",
    "- unique tweet ID, 8 bytes\n",
    "- user_id, 32 bytes\n",
    "- tweet text, 140 charcters, 140 bytes\n",
    "- media, 10KB avg (we can constrain this by formatting images)\n",
    "total: 10KB (bounded by media size)\n",
    "\n",
    "- 10KB * 500M/day = 5TB per day or 150TB per month\n",
    "- 250B read requests a month * 400 req. per second / 1 billion req per month = 100,000 requests a second\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### data structures\n",
    "\n",
    "\n",
    "There are two approaches to the data structure here, a pull and a push system.\n",
    "\n",
    "In the push system, we keep track of a user and all of their followers, and their messages, and additionally a user and their feed.\n",
    "\n",
    "### push archetecture\n",
    "\n",
    "In this method, when a new message is published by a user, we need to look up their followers and push the message to the followers feeds.\n",
    "\n",
    "User followers\n",
    "\n",
    "    User, follower, metadata\n",
    "    \n",
    "There is a inque row for each (user, follower) pair, meaning a user has many rows, one for each follower.\n",
    "\n",
    "User feed\n",
    "\n",
    "    User, message_id\n",
    "\n",
    "There is a unique row for each (user, message_id) and we construct the feed by selecting all the rows with that user.\n",
    "\n",
    "Messages\n",
    "\n",
    "    message_id, message, metadata\n",
    "    \n",
    "The problem here is that if a user is very popular, i.e. has millions of followers, publishing to followers feed list will involve millions of writes.  The good part of this approach is that constructing feeds is realatively cheep.  We simply get their messages\n",
    "\n",
    "Notice I haven't mentioned timestamps, or message sorting.  We can include the message timestamp in the message table row, and order them, or we can build time into the order of the message_ids to save some time in sorting.  The second approach works straight away if we are constructing a purely time-ordered feed.  If we have some kind of formula or model for relevance, and order by relevance, we need to have access to timestamps, which we can either keep in the message table as a column, or we can build a system for extracting the timestamp from the message id.\n",
    "\n",
    "\n",
    "### pull archetecture\n",
    "\n",
    "In the pull system, involves keeping track of all the users a user follows.\n",
    "\n",
    "User following\n",
    "\n",
    "    User, User (or user list)\n",
    "    \n",
    "Where for every user we keep track of who they're following.\n",
    "\n",
    "User messages\n",
    "\n",
    "    User, message_id\n",
    "\n",
    "Messages\n",
    "\n",
    "    message_id, message, metadata\n",
    "\n",
    "Now, to construct feeds, we need to get all the other users a user is following, then grab their messages and put them in order.\n",
    "Encoding message timestamp in the message_id helps here too.\n",
    "\n",
    "### Performance improvements\n",
    "\n",
    "To speed up the gathering of messages, we can keep a message cache (LRU or something similar).  This will help us with very popular messages and very recent messages.\n",
    "\n",
    "To improve experience at log-in, we can keep a table of every user's most recent feed, and get it immediately on log-in.  We'll set a size limit of this feed to keep.  This will get users old messages already in their feed, and allow us some time to contruct a feed of new messages (or old feed messages if they're looking backwards).\n",
    "\n",
    "Deleted messages are handles by dropping them when we try to look them up.  For the cached feeds, we can validate the messages (see if they still exist, but not read them entirely.  If they don't exist anymore, we can drop them from the cached feed before returning them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
