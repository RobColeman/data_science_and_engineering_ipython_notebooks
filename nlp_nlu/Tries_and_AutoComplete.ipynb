{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rcoleman/rob/code/python/data_science_ipython_notebooks/data\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math, sys, os\n",
    "from numpy.random import randn\n",
    "\n",
    "PROJECT_HOME = os.environ.get('PROJECT_HOME', None)\n",
    "sys.path.insert(0, PROJECT_HOME + \"/util\")\n",
    "from loaders import get_english_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trie_dict:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._end = '_end_'\n",
    "        self._root = dict()\n",
    "    \n",
    "    def insert(self, word):\n",
    "        current_dict = self._root\n",
    "        for letter in word:\n",
    "            current_dict = current_dict.setdefault(letter, {})\n",
    "        current_dict[self._end] = self._end\n",
    "    \n",
    "    def insert_batch(self, words):\n",
    "        for word in words:\n",
    "            self.insert(word)\n",
    "    \n",
    "    def view(self):\n",
    "        print self._root\n",
    "    \n",
    "    def view_root_keys(self):\n",
    "        print self._root.keys()\n",
    "    \n",
    "    def contains(self, word):\n",
    "        current_dict = self._root\n",
    "        for letter in word:\n",
    "            if letter in current_dict:\n",
    "                current_dict = current_dict[letter]\n",
    "            else:\n",
    "                return False\n",
    "        # the _end flag indicates this is the end of a word\n",
    "        # if it's not there, the word continues\n",
    "        if self._end in current_dict:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "\n",
    "    def suggest(self, partial, limit = 5):\n",
    "        \"\"\"\n",
    "        Since this trie doesn't store frequency of words as it trains, we're just \n",
    "        going to return the alphabetically first 'limit', shortest, terms.\n",
    "        \"\"\"\n",
    "        suggestions = []\n",
    "\n",
    "        def suggest_dfs(partial_dict, partial ):\n",
    "                if len(suggestions) < limit:\n",
    "                    for ch in sorted(partial_dict.keys()): \n",
    "                        # sorting by alpha, this happens to give us _end_ first\n",
    "                        # could be pre-sorting by frequency for better \n",
    "                        #   speed and smarted recommendations\n",
    "                        if len(suggestions) >= limit:\n",
    "                            break\n",
    "                        elif ch == self._end:\n",
    "                            suggestions.append(partial)\n",
    "                        else:\n",
    "                            # recurse\n",
    "                            suggest_dfs(partial_dict[ch], partial + ch)\n",
    "\n",
    "        partial_dict = self._find_patial(partial)\n",
    "        if not partial_dict == None:\n",
    "            suggest_dfs(partial_dict, partial)\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "    def _find_patial(self, partial):\n",
    "        top_dict = self._root\n",
    "        for char in partial:\n",
    "            if char in top_dict:\n",
    "                top_dict = top_dict[char]\n",
    "            else:\n",
    "                # there are no words starting with this sequence\n",
    "                return None\n",
    "        return top_dict\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 21] Is a directory: '/Users/rcoleman/rob/code/python/data_science_ipython_notebooks/nlp_nlu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5f7369f04782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrie_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_english_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/rcoleman/rob/code/python/data_science_ipython_notebooks/util/loaders.py\u001b[0m in \u001b[0;36mget_english_dictionary\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_english_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/util\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/data/text/words/en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 21] Is a directory: '/Users/rcoleman/rob/code/python/data_science_ipython_notebooks/nlp_nlu'"
     ]
    }
   ],
   "source": [
    "trie = Trie_dict()\n",
    "trie.insert_batch(get_english_dictionary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggestions\n",
      "\n",
      "'reac': \n",
      "['reacceptance', 'reaccess', 'reaccession', 'reacclimatization', 'reacclimatize']\n",
      "\n",
      "'poo': \n",
      "['pooa', 'pooch', 'pooder', 'poodle', 'poodledom']\n",
      "\n",
      "'whal': \n",
      "['whale', 'whaleback', 'whalebacker', 'whalebird', 'whaleboat']\n",
      "\n",
      "'dan': \n",
      "['dan', 'danaid', 'danaide', 'danaine', 'danaite']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"suggestions\"\n",
    "print \"\"\n",
    "print \"'reac': \"\n",
    "print trie.suggest(\"reac\")\n",
    "print \"\"\n",
    "print \"'poo': \"\n",
    "print trie.suggest(\"poo\")\n",
    "print \"\"\n",
    "print \"'whal': \"\n",
    "print trie.suggest(\"whal\")\n",
    "print \"\"\n",
    "print \"'dan': \"\n",
    "print trie.suggest(\"dan\")\n",
    "print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tries with some statistical flavor\n",
    "\n",
    "## Trie with frequency distribution\n",
    "Create a Trie where we keep track of how many times we've gone down each branch of the tree.  We can use this distribution over suggestions to rank our suggestions.\n",
    "\n",
    "This prob. can be expressed as P( next_word = word_i | incomplete)\n",
    "\n",
    "## Trie with simple Markov-Transition Distribution\n",
    "We can use some sentance context to make suggestions as well.  We can build a transition matrix from work X to work Y (represented sparsely because the # of words is likely huge), to get the probability of the the next word, given the last word, or \n",
    "\n",
    "P( next_word = word_i | incomplete, last_word = word_j) = P( next_word = word_i | incomplete) * P( next_word = word_i | last_word = word_j )\n",
    "\n",
    "## HMMs\n",
    "We can extend the Markov toolkit even further, by modeling the word sequence as a Hidden-Markov Model.  The Hidden-Markov model creates a tractible way of computing not just P( next_word = word_i | last_word = word_j ) but P( next_word = word_i | last_word = word_j, last_last_word = word_j, ..., all the way to firs_word = word_x ).\n",
    "\n",
    "HMMs are a whole different beast, but once you've got one, you can update your ranking of the next word with the following:\n",
    "\n",
    "P( next_word = word_i | incomplete, all_previous_words) = P( next_word = word_i | incomplete) * P( next_word = word_i | all_previous_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
