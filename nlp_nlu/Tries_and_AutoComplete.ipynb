{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math, sys, os\n",
    "from numpy.random import randn\n",
    "\n",
    "PROJECT_HOME = os.environ.get('PROJECT_HOME', None)\n",
    "sys.path.insert(0, PROJECT_HOME + \"/util\")\n",
    "from loaders import get_english_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trie_dict:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._end = '_end_'\n",
    "        self._root = dict()\n",
    "    \n",
    "    def insert(self, word):\n",
    "        current_dict = self._root\n",
    "        for letter in word:\n",
    "            current_dict = current_dict.setdefault(letter, {})\n",
    "        current_dict[self._end] = self._end\n",
    "    \n",
    "    def insert_batch(self, words):\n",
    "        for word in words:\n",
    "            self.insert(word)\n",
    "    \n",
    "    def view(self):\n",
    "        print(self._root)\n",
    "    \n",
    "    def view_root_keys(self):\n",
    "        print(self._root.keys())\n",
    "    \n",
    "    def contains(self, word):\n",
    "        current_dict = self._root\n",
    "        for letter in word:\n",
    "            if letter in current_dict:\n",
    "                current_dict = current_dict[letter]\n",
    "            else:\n",
    "                return False\n",
    "        # the _end flag indicates this is the end of a word\n",
    "        # if it's not there, the word continues\n",
    "        if self._end in current_dict:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "\n",
    "    def suggest(self, partial, limit = 5):\n",
    "        \"\"\"\n",
    "        Since this trie doesn't store frequency of words as it trains, we're just \n",
    "        going to return the alphabetically first 'limit', shortest, terms.\n",
    "        \"\"\"\n",
    "        suggestions = []\n",
    "\n",
    "        def suggest_dfs(partial_dict, partial ):\n",
    "                if len(suggestions) < limit:\n",
    "                    for ch in sorted(partial_dict.keys()): \n",
    "                        # sorting by alpha, this happens to give us _end_ first\n",
    "                        # could be pre-sorting by frequency for better \n",
    "                        #   speed and smarted recommendations\n",
    "                        if len(suggestions) >= limit:\n",
    "                            break\n",
    "                        elif ch == self._end:\n",
    "                            suggestions.append(partial)\n",
    "                        else:\n",
    "                            # recurse\n",
    "                            suggest_dfs(partial_dict[ch], partial + ch)\n",
    "\n",
    "        partial_dict = self._find_patial(partial)\n",
    "        if not partial_dict == None:\n",
    "            suggest_dfs(partial_dict, partial)\n",
    "        \n",
    "        return suggestions\n",
    "\n",
    "    def _find_patial(self, partial):\n",
    "        top_dict = self._root\n",
    "        for char in partial:\n",
    "            if char in top_dict:\n",
    "                top_dict = top_dict[char]\n",
    "            else:\n",
    "                # there are no words starting with this sequence\n",
    "                return None\n",
    "        return top_dict\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A note on the dictionary.set_default(key, default_val) method.  \n",
    "# This method is equivilant to a method that looks like this:\n",
    "def set_default(dictionary, key, default_val = {}):\n",
    "    if key in dictionary:\n",
    "        return dictionary[key]\n",
    "    else:\n",
    "        dictionary[key] = default_val\n",
    "        return dictionary[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trie = Trie_dict()\n",
    "trie.insert_batch(get_english_dictionary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestions\n",
      "\n",
      "'reac': \n",
      "['reacceptance', 'reaccess', 'reaccession', 'reacclimatization', 'reacclimatize']\n",
      "\n",
      "'poo': \n",
      "['pooa', 'pooch', 'pooder', 'poodle', 'poodledom']\n",
      "\n",
      "'whal': \n",
      "['whale', 'whaleback', 'whalebacker', 'whalebird', 'whaleboat']\n",
      "\n",
      "'dan': \n",
      "['dan', 'danaid', 'danaide', 'danaine', 'danaite']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Suggestions\")\n",
    "print(\"\")\n",
    "print(\"'reac': \")\n",
    "print(trie.suggest(\"reac\"))\n",
    "print( \"\")\n",
    "print( \"'poo': \")\n",
    "print( trie.suggest(\"poo\"))\n",
    "print( \"\")\n",
    "print( \"'whal': \")\n",
    "print( trie.suggest(\"whal\"))\n",
    "print( \"\")\n",
    "print( \"'dan': \")\n",
    "print( trie.suggest(\"dan\"))\n",
    "print( \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tries with some statistical flavor\n",
    "\n",
    "## Trie with frequency distribution\n",
    "Create a Trie where we keep track of how many times we've gone down each branch of the tree.  We can use this distribution over suggestions to rank our suggestions.\n",
    "\n",
    "This prob. can be expressed as P( next_word = word_i | incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trie_Statistical:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._end = '_end_'\n",
    "        self._root = dict()\n",
    "        self._total_words = 0\n",
    "        self._search_limit = 100\n",
    "    \n",
    "    def insert(self, word):\n",
    "        current_dict = self._root\n",
    "        for letter in word:\n",
    "            current_dict = current_dict.setdefault(letter, {})\n",
    "        # keep counts at { last_letter : {'_end_' : count} }\n",
    "        if self._end in current_dict:\n",
    "            current_dict[self._end] += 1\n",
    "        else:\n",
    "            current_dict[self._end] = 1\n",
    "        self._total_words += 1\n",
    "        \n",
    "    \n",
    "    def insert_batch(self, words):\n",
    "        for word in words:\n",
    "            self.insert(word)\n",
    "    \n",
    "    def view(self):\n",
    "        print(self._root)\n",
    "    \n",
    "    def view_root_keys(self):\n",
    "        print(self._root.keys())\n",
    "    \n",
    "    def _normalize_suggestion_probs(self, suggestions):\n",
    "        total = 0\n",
    "        for w, c in suggestions:\n",
    "            total += c\n",
    "        for i, t in enumerate(suggestions):\n",
    "            suggestions[i] = (t[0], t[1] / total)\n",
    "    \n",
    "    def contains(self, word):\n",
    "        current_dict = self._root\n",
    "        for letter in word:\n",
    "            if letter in current_dict:\n",
    "                current_dict = current_dict[letter]\n",
    "            else:\n",
    "                return False\n",
    "        # the _end flag indicates this is the end of a word\n",
    "        # if it's not there, the word continues\n",
    "        if self._end in current_dict:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def suggest(self, partial, limit = 5):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        suggestions = []\n",
    "\n",
    "        def suggest_dfs(partial_dict, partial ):\n",
    "                if len(suggestions) < self._search_limit:\n",
    "                    for ch in sorted(partial_dict.keys()): \n",
    "                        # sorting by alpha, this happens to give us _end_ first\n",
    "                        # could be pre-sorting by frequency for better \n",
    "                        #   speed and smarter recommendations\n",
    "                        if len(suggestions) >= self._search_limit:\n",
    "                            break\n",
    "                        elif ch == self._end:\n",
    "                            suggestions.append((partial, partial_dict[self._end]))\n",
    "                        else:\n",
    "                            # recurse\n",
    "                            suggest_dfs(partial_dict[ch], partial + ch)\n",
    "\n",
    "        partial_dict = self._find_patial(partial)\n",
    "        if not partial_dict == None:\n",
    "            suggest_dfs(partial_dict, partial)\n",
    "        \n",
    "        self._normalize_suggestion_probs(suggestions)\n",
    "        sorted_suggestions = sorted(suggestions, key=lambda pair: pair[1])\n",
    "        if limit > 0:\n",
    "            return sorted_suggestions[:limit]\n",
    "        else:\n",
    "            return sorted_suggestions\n",
    "\n",
    "\n",
    "    def _find_patial(self, partial):\n",
    "        top_dict = self._root\n",
    "        for char in partial:\n",
    "            if char in top_dict:\n",
    "                top_dict = top_dict[char]\n",
    "            else:\n",
    "                # there are no words starting with this sequence\n",
    "                return None\n",
    "        return top_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trie = Trie_Statistical()\n",
    "# we're reading a dictionary, so we will have 1 example of every word.\n",
    "trie.insert_batch(get_english_dictionary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestions\n",
      "\n",
      "'reac': \n",
      "[('reacceptance', 0.01639344262295082), ('reaccess', 0.01639344262295082), ('reaccession', 0.01639344262295082), ('reacclimatization', 0.01639344262295082), ('reacclimatize', 0.01639344262295082)]\n",
      "\n",
      "'poo': \n",
      "[('pooa', 0.023255813953488372), ('pooch', 0.023255813953488372), ('pooder', 0.023255813953488372), ('poodle', 0.023255813953488372), ('poodledom', 0.023255813953488372)]\n",
      "\n",
      "'whal': \n",
      "[('whale', 0.047619047619047616), ('whaleback', 0.047619047619047616), ('whalebacker', 0.047619047619047616), ('whalebird', 0.047619047619047616), ('whaleboat', 0.047619047619047616)]\n",
      "\n",
      "'dan': \n",
      "[('dan', 0.013888888888888888), ('danaid', 0.013888888888888888), ('danaide', 0.013888888888888888), ('danaine', 0.013888888888888888), ('danaite', 0.013888888888888888)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Suggestions\")\n",
    "print(\"\")\n",
    "print(\"'reac': \")\n",
    "print(trie.suggest(\"reac\"))\n",
    "print( \"\")\n",
    "print( \"'poo': \")\n",
    "print( trie.suggest(\"poo\"))\n",
    "print( \"\")\n",
    "print( \"'whal': \")\n",
    "print( trie.suggest(\"whal\"))\n",
    "print( \"\")\n",
    "print( \"'dan': \")\n",
    "print( trie.suggest(\"dan\"))\n",
    "print( \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More advanced Tries\n",
    "\n",
    "## Trie with simple Markov-Transition Distribution\n",
    "We can use some sentance context to make suggestions as well.  We can build a transition matrix from work X to work Y (represented sparsely because the # of words is likely huge), to get the probability of the the next word, given the last word, or \n",
    "\n",
    "P( next_word = word_i | incomplete, last_word = word_j) = P( next_word = word_i | incomplete) * P( next_word = word_i | last_word = word_j )\n",
    "\n",
    "## HMMs\n",
    "We can extend the Markov toolkit even further, by modeling the word sequence as a Hidden-Markov Model.  The Hidden-Markov model creates a tractible way of computing not just P( next_word = word_i | last_word = word_j ) but P( next_word = word_i | last_word = word_j, last_last_word = word_j, ..., all the way to firs_word = word_x ).\n",
    "\n",
    "HMMs are a whole different beast, but once you've got one, you can update your ranking of the next word with the following:\n",
    "\n",
    "P( next_word = word_i | incomplete, all_previous_words) = P( next_word = word_i | incomplete) * P( next_word = word_i | all_previous_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
