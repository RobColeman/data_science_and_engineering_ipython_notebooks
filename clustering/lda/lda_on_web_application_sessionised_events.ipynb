{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation on Web-App events\n",
    "\n",
    "LDA is a clustering model often applied to topic modeling.  Roughly LDA uses a graphical model of nested multinomials to destribe the distribution of tokens (words) within topics, and topics within documents.  \n",
    "\n",
    "Gaphics for the model's mathematics is left as a TODO.  For now [LDA Wikipedia Article](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
    "\n",
    "However, this nested multinomial structure can be applied to other settings as well.  In this case, if we are interested in clustering behavior withing a web-application. If we are minitoring unique events within the application, and we assume that users of the application come to the app with some underlaying purpose for each session (uninterupted period of use) in the application (the cardinality of total purposes for sessions being much smaller than that of events), we can cluster sesssions by event frequency the same way we would cluster documents by word frequency.  Structurally, the session-event model can be equivilent to the topic-word model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Online-LDA\n",
    "\n",
    "import sys, re, time, string\n",
    "import numpy as n\n",
    "from scipy.special import gammaln, psi\n",
    "\n",
    "n.random.seed(100000001)\n",
    "meanchangethresh = 0.001\n",
    "\n",
    "def dirichlet_expectation(alpha):\n",
    "    \"\"\"\n",
    "    For a vector theta ~ Dir(alpha), computes E[log(theta)] given alpha.\n",
    "    \"\"\"\n",
    "    if (len(alpha.shape) == 1):\n",
    "        return(psi(alpha) - psi(n.sum(alpha)))\n",
    "    return(psi(alpha) - psi(n.sum(alpha, 1))[:, n.newaxis])\n",
    "\n",
    "def parse_sessions_list(sessions, event_set):\n",
    "\n",
    "    D = len(sessions)\n",
    "    \n",
    "    eventsids = list()\n",
    "    eventscts = list()\n",
    "    for D in range(0, D):\n",
    "        events = sessions[D]\n",
    "        ddict = dict()\n",
    "        for e in events:\n",
    "            if (e in event_set):\n",
    "                eventtoken = event_set[e]\n",
    "                if (not eventtoken in ddict):\n",
    "                    ddict[eventtoken] = 0\n",
    "                ddict[eventtoken] += 1\n",
    "        eventsids.append(ddict.keys())\n",
    "        eventscts.append(ddict.values())\n",
    "\n",
    "    return((eventsids, eventscts))\n",
    "\n",
    "class OnlineLDA:\n",
    "    \"\"\"\n",
    "    Implements online VB for LDA as described in (Hoffman et al. 2010).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, event_set, K, D, alpha = None, eta = None, tau0 = 1024, kappa = 0.7):\n",
    "        self._events = dict()\n",
    "        for events in event_set:\n",
    "            events = events.lower()\n",
    "            self._events[events] = len(self._events)\n",
    "\n",
    "        self._K = K\n",
    "        self._W = len(self._events)\n",
    "        self._D = D\n",
    "        self._alpha = alpha if alpha else 1.0 / K\n",
    "        self._eta = eta if eta else 1.0 / K\n",
    "        self._tau0 = tau0 + 1\n",
    "        self._kappa = kappa\n",
    "        self._updatect = 0\n",
    "\n",
    "        # Initialize the variational distribution q(beta|lambda)\n",
    "        self._lambda = 1 * n.random.gamma(100.0, 1.0 / 100.0, (self._K, self._W))\n",
    "        self._Elogbeta = dirichlet_expectation(self._lambda)\n",
    "        self._expElogbeta = n.exp(self._Elogbeta)\n",
    "\n",
    "    def update_lambda(self, sessions):\n",
    "\n",
    "        # rhot will be between 0 and 1, and says how much to weight\n",
    "        # the information we got from this mini-batch.\n",
    "        rhot = pow(self._tau0 + self._updatect, -self._kappa)\n",
    "        self._rhot = rhot\n",
    "        # Do an E step to update gamma, phi | lambda for this\n",
    "        # mini-batch. This also returns the information about phi that\n",
    "        # we need to update lambda.\n",
    "        (gamma, sstats) = self.do_e_step(sessions)\n",
    "        # Estimate held-out likelihood for current values of lambda.\n",
    "        bound = self.approx_bound(sessions, gamma)\n",
    "        # Update lambda based on documents.\n",
    "        self._lambda = self._lambda * (1-rhot) + \\\n",
    "            rhot * (self._eta + self._D * sstats / len(sessions))\n",
    "        self._Elogbeta = dirichlet_expectation(self._lambda)\n",
    "        self._expElogbeta = n.exp(self._Elogbeta)\n",
    "        self._updatect += 1\n",
    "\n",
    "        return(gamma, bound)\n",
    "\n",
    "    def approx_bound(self, sessions, gamma):\n",
    "        \"\"\"\n",
    "        Estimates the variational bound over *all documents* using only\n",
    "        the documents passed in as \"sessions.\" gamma is the set of parameters\n",
    "        to the variational distribution q(theta) corresponding to the\n",
    "        set of documents passed in.\n",
    "\n",
    "        The output of this function is going to be noisy, but can be\n",
    "        useful for assessing convergence.\n",
    "        \"\"\"\n",
    "\n",
    "        # This is to handle the case where someone just hands us a single\n",
    "        # document, not in a list.\n",
    "        if (type(sessions).__name__ == 'string'):\n",
    "            temp = list()\n",
    "            temp.append(sessions)\n",
    "            sessions = temp\n",
    "\n",
    "        (eventsids, eventscts) = parse_sessions_list(sessions, self._events)\n",
    "        batchD = len(sessions)\n",
    "\n",
    "        score = 0\n",
    "        Elogtheta = dirichlet_expectation(gamma)\n",
    "        expElogtheta = n.exp(Elogtheta)\n",
    "\n",
    "        # E[log p(sessions | theta, id)]\n",
    "        for d in range(0, batchD):\n",
    "            gammad = gamma[d, :]\n",
    "            ids = eventsids[d]\n",
    "            cts = n.array(eventscts[d])\n",
    "            phinorm = n.zeros(len(ids))\n",
    "\n",
    "            for i in range(0, len(ids)):\n",
    "                temp = Elogtheta[d, :] + self._Elogbeta[:, ids[i]]\n",
    "                tmax = max(temp)\n",
    "                phinorm[i] = n.log(sum(n.exp(temp - tmax))) + tmax\n",
    "            score += n.sum(cts * phinorm)\n",
    "#             oldphinorm = phinorm\n",
    "#             phinorm = n.dot(expElogtheta[d, :], self._expElogbeta[:, ids])\n",
    "#             print oldphinorm\n",
    "#             print n.log(phinorm)\n",
    "#             score += n.sum(cts * n.log(phinorm))\n",
    "\n",
    "        # E[log p(theta | alpha) - log q(theta | gamma)]\n",
    "        score += n.sum((self._alpha - gamma)*Elogtheta)\n",
    "        score += n.sum(gammaln(gamma) - gammaln(self._alpha))\n",
    "        score += sum(gammaln(self._alpha*self._K) - gammaln(n.sum(gamma, 1)))\n",
    "\n",
    "        # Compensate for the subsampling of the population of documents\n",
    "        score = score * self._D / len(sessions)\n",
    "\n",
    "        # E[log p(beta | eta) - log q (beta | lambda)]\n",
    "        score = score + n.sum((self._eta-self._lambda)*self._Elogbeta)\n",
    "        score = score + n.sum(gammaln(self._lambda) - gammaln(self._eta))\n",
    "        score = score + n.sum(gammaln(model._eta*model._W) - \n",
    "                              gammaln(n.sum(model._lambda, 1)))\n",
    "\n",
    "        return (score)\n",
    "\n",
    "\n",
    "    def do_e_step(self, sessions):\n",
    "        # This is to handle the case where someone just hands us a single\n",
    "        # document, not in a list.\n",
    "        if (type(sessions).__name__ == 'string'):\n",
    "            temp = list()\n",
    "            temp.append(sessions)\n",
    "            sessions = temp\n",
    "\n",
    "        (eventsids, eventscts) = parse_sessions_list(sessions, self._events)\n",
    "        batchD = len(sessions)\n",
    "\n",
    "        # Initialize the variational distribution q(theta|gamma) for\n",
    "        # the mini-batch\n",
    "        gamma = 1*n.random.gamma(100., 1./100., (batchD, self._K))\n",
    "        Elogtheta = dirichlet_expectation(gamma)\n",
    "        expElogtheta = n.exp(Elogtheta)\n",
    "\n",
    "        sstats = n.zeros(self._lambda.shape)\n",
    "        # Now, for each document d update that document's gamma and phi\n",
    "        it = 0\n",
    "        meanchange = 0\n",
    "        for d in range(0, batchD):\n",
    "            # These are mostly just shorthand (but might help cache locality)\n",
    "            ids = eventsids[d]\n",
    "            cts = eventscts[d]\n",
    "            gammad = gamma[d, :]\n",
    "            Elogthetad = Elogtheta[d, :]\n",
    "            expElogthetad = expElogtheta[d, :]\n",
    "            expElogbetad = self._expElogbeta[:, ids]\n",
    "            # The optimal phi_{dwk} is proportional to \n",
    "            # expElogthetad_k * expElogbetad_w. phinorm is the normalizer.\n",
    "            phinorm = n.dot(expElogthetad, expElogbetad) + 1e-100\n",
    "            # Iterate between gamma and phi until convergence\n",
    "            for it in range(0, 100):\n",
    "                lastgamma = gammad\n",
    "                # We represent phi implicitly to save memory and time.\n",
    "                # Substituting the value of the optimal phi back into\n",
    "                # the update for gamma gives this update. Cf. Lee&Seung 2001.\n",
    "                gammad = self._alpha + expElogthetad * \\\n",
    "                    n.dot(cts / phinorm, expElogbetad.T)\n",
    "                Elogthetad = dirichlet_expectation(gammad)\n",
    "                expElogthetad = n.exp(Elogthetad)\n",
    "                phinorm = n.dot(expElogthetad, expElogbetad) + 1e-100\n",
    "                # If gamma hasn't changed much, we're done.\n",
    "                meanchange = n.mean(abs(gammad - lastgamma))\n",
    "                if (meanchange < meanchangethresh):\n",
    "                    break\n",
    "            gamma[d, :] = gammad\n",
    "            # Contribution of document d to the expected sufficient\n",
    "            # statistics for the M step.\n",
    "            sstats[:, ids] += n.outer(expElogthetad.T, cts/phinorm)\n",
    "\n",
    "        # This step finishes computing the sufficient statistics for the\n",
    "        # M step, so that\n",
    "        # sstats[k, w] = \\sum_d n_{dw} * phi_{dwk} \n",
    "        # = \\sum_d n_{dw} * exp{Elogtheta_{dk} + Elogbeta_{kw}} / phinorm_{dw}.\n",
    "        sstats = sstats * self._expElogbeta\n",
    "\n",
    "        return((gamma, sstats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle, string, numpy, getopt, sys, random, time, re, pprint\n",
    "\n",
    "import mongo_client\n",
    "from bson import ObjectId\n",
    "\n",
    "project_id = \"517eda23c82561f72a000005\"\n",
    "\n",
    "# The number of documents to analyze each k\n",
    "batchsize = 500\n",
    "# The total number of documents in Wikipedia\n",
    "D = mongo_client.get_session_count(project_id)\n",
    "# The number of topics\n",
    "K = 6\n",
    "\n",
    "event_set = mongo_client.get_events_ids_by_project_id(project_id)\n",
    "W = len(event_set)\n",
    "model = onlineldavb.OnlineLDA(event_set, K, D)\n",
    "\n",
    "for k, (n_skip,n_limit) in enumerate(build_batches(n, batch_size)):\n",
    "\n",
    "    sessions = mongo_client.get_sessions_batch(project_id, n_skip, n_limit)\n",
    "        \n",
    "    (gamma, bound) = model.update_lambda(sessions)\n",
    "\n",
    "    (event_tokens, event_counts) = onlineldavb.parse_sessions_list(sessions, model._event_set)\n",
    "    pereventsbound = bound * len(sessions) / (D * sum(map(sum, event_counts)))\n",
    "\n",
    "    print '%d:  rho_t = %f,  held-out perplexity estimate = %f' % \\\n",
    "        (k, model._rhot, numpy.exp(-pereventsbound))\n",
    "\n",
    "    if (k % 10 == 0):\n",
    "        numpy.savetxt('lambda-%d.dat' % k, model._lambda)\n",
    "        numpy.savetxt('gamma-%d.dat' % k, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
